{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "class Parser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.punctuation = list(string.punctuation)\n",
    "        \n",
    "    '''\n",
    "    def parse(self, email_path):\n",
    "        \"\"\"Parse an email.\"\"\"\n",
    "        with open(email_path, errors='ignore') as e:\n",
    "            msg = email.message_from_file(e)\n",
    "        return None if not msg else self.get_email_content(msg)\n",
    "\n",
    "    def get_email_content(self, msg):\n",
    "        \"\"\"Extract the email content.\"\"\"\n",
    "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else []\n",
    "        body = self.get_email_body(msg.get_payload(),\n",
    "                                   msg.get_content_type())\n",
    "        content_type = msg.get_content_type()\n",
    "        # Returning the content of the email\n",
    "        return {\"subject\": subject,\n",
    "                \"body\": body,\n",
    "                \"content_type\": content_type}\n",
    "                \n",
    "    def get_email_body(self, payload, content_type):\n",
    "        \"\"\"Extract the body of the email.\"\"\"\n",
    "        body = []\n",
    "        if type(payload) is str and content_type == 'text/plain':\n",
    "            return self.tokenize(payload)\n",
    "        elif type(payload) is str and content_type == 'text/html':\n",
    "            return self.tokenize(strip_tags(payload))\n",
    "        elif type(payload) is list:\n",
    "            for p in payload:\n",
    "                body += self.get_email_body(p.get_payload(),\n",
    "                                            p.get_content_type())\n",
    "        return body\n",
    "    '''\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Transform a text string in tokens. Perform two main actions,\n",
    "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
    "        for c in self.punctuation:\n",
    "            text = text.replace(c, \"\")\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        tokens = list(filter(None, text.split(\" \")))\n",
    "        # Stemming of the tokens\n",
    "        return [self.stemmer.stem(w) for w in tokens if w not in self.stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = Parser()\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_text(my_text):\n",
    "    my_text = parse.tokenize(my_text)\n",
    "    my_o = \"\"\n",
    "    for i in my_text:\n",
    "        my_o += \" \" + i\n",
    "    return my_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_1 = \"I enjoy reading books in my cozy corner.\"\n",
    "txt_2 = \"She enjoy to explore new novels at the local bookstore.\"\n",
    "txt_3 = \"Reading books is a wonderful way to relax and unwind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "txt_1_reduce = parse.tokenize(txt_1)\n",
    "txt_2_reduce = reduce_text(txt_2)\n",
    "txt_3_reduce = reduce_text(txt_3)\n",
    "txt_secundary = [txt_2_reduce, txt_3_reduce]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =vectorizer.fit_transform([txt_1,txt_2,txt_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enjoy': 6,\n",
       " 'reading': 14,\n",
       " 'books': 2,\n",
       " 'in': 8,\n",
       " 'my': 11,\n",
       " 'cozy': 5,\n",
       " 'corner': 4,\n",
       " 'she': 16,\n",
       " 'to': 18,\n",
       " 'explore': 7,\n",
       " 'new': 12,\n",
       " 'novels': 13,\n",
       " 'at': 1,\n",
       " 'the': 17,\n",
       " 'local': 10,\n",
       " 'bookstore': 3,\n",
       " 'is': 9,\n",
       " 'wonderful': 21,\n",
       " 'way': 20,\n",
       " 'relax': 15,\n",
       " 'and': 0,\n",
       " 'unwind': 19}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
